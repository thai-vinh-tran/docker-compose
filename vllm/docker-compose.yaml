vllm:
  profiles: ['service']
  container_name: vllm
  image: vllm/vllm-openai:latest
  entrypoint: ["python3", "-m", "vllm.entrypoints.openai.api_server", "--model", "meta-llama/Llama-3.1-8B-Instruct", "--tensor_parallel_size", "2", "--host", "0.0.0.0", "--port", "8000", "--max_num_seqs", "64", "--enable-auto-tool-choice", "--tool-call-parser", "llama3_json", "--chat-template", "examples/tool_chat_template_llama3.1_json.jinja"]
  environment:
    - CUDA_VISIBLE_DEVICES=0,1
  deploy:
    resources:
      reservations:
        devices:
        - driver: nvidia
          device_ids: ['0', '1']
          capabilities: [gpu]
  ports:
    - "8000:8000"
  restart: always
  healthcheck:
    test: [ "CMD", "curl", "-f", "http://0.0.0.0:8000/v1/models" ]
    interval: 30s
    timeout: 5s
    retries: 20
  volumes:
    - type: bind
      source: /root/.cache
      target: /root/.cache
  shm_size: '64gb'
  networks:
    - vllm